# CRNCH Summit Poster Session
The CRNCH Student Poster Session will be held on February 8th, 2024 from 4:40 - 6 PM. We are excited to have a large number of posters from our CRNCH students, and we also are thrilled to be able to feature the work of some of our [CRNCH PhD Fellowship winners](https://crnch.gatech.edu/content/crnch-fellowship).

We note that in some cases, we have not linked posters due to work being under submission or at the presenter's request. 

### 2023-2024 Fellowship Winners

| Student Presenter, Student Authors | Poster Title | Advisor(s) | GT Department | [Poster] [Abstract] |
| ---------------------------------- | ------------ | ---------- | -------------|-------------------|


| Student Presenter, Student Authors | Poster Title | Advisor(s) | GT Department | [Poster] [Abstract] |
| ---------------------------------- | ------------ | ---------- | -------------|-------------------|
| Yongnuo Yang, Jeremy Wang, Alexander Contratti | Efficiency and Parallelism on the Lucata Pathfinder with the HPCG Benchmark | Jeffrey Young, Aaron Jezghani | ECE, SCS | [Poster]() [Abstract](#yy) |


## Student Abstracts:

<a id="yy">**Yongnuo Yang, Jeremy Wang, Alexander Contratti - "Efficiency and Parallelism on the Lucata Pathfinder with the HPCG Benchmark"**</a>

In a post-Moore era of computing, near-memory architecture offers an alternative path for continuing to improve processor performance. By placing compute elements closer to memory and forgoing a cache hierarchy, near-memory systems address the memory wall problem, where a large disparity exists between processor speed and memory access. The Lucata Pathfinder platform is a multi-node, massively multithreaded system that uses lightweight migratory threads across a Partitioned Global Address Space such that every memory access occurs locally. It is especially well suited for applications involving sparse matrices, where caches struggle, such as the High Performance Conjugate Gradient (HPCG) benchmark. HPCG applies the Conjugate Gradient algorithm to a generated problem and consists of four main mathematical operations: dot product (DDOT), weighted vector sum (WAXPBY), sparse matrix-vector multiplication (SPMV), and the symmetric Gauss-Seidel method (SYMGS). Through program runs on the Pathfinder and emusim, a simulated environment for the Pathfinder, the performance and parallelization of the system are assessed for each of the operations and the benchmark as a whole. The Pathfinder achieves good utilization and load distribution for smaller problem sizes of HPCG. Further work is being done for larger problem sizes to test the scalability and power efficiency of the system.

<a id="cl">**Chaojian Li - On-Device AR/VR 3D Reconstruction and Rendering"**</a>

"Neural Radiance Field (NeRF) based 3D reconstruction is highly desirable for immersive Augmented and Virtual Reality (AR/VR) applications, but achieving instant (i.e., &lt; 5 seconds) on-device NeRF training and real-time (i.e., &gt; 30 FPS) high-quality NeRF rendering remains a challenge. In this work, we first identify the inefficiency bottleneck during NeRF training: the need to interpolate NeRF embeddings up to 200,000 times from a 3D embedding grid during each training iteration. To alleviate this, we propose an algorithm-hardware co-design acceleration framework that achieves instant on-device NeRF training. Our algorithm decomposes the embedding grid representation in terms of color and density. Our hardware accelerator further reduces the dominant memory accesses for embedding grid interpolation. Moreover, we develop a web-based real-time NeRF rendering viewer to allow the reconstructed scenes can be viewed in cross-platform devices even with out the proposed hardware accelerator."

<a id="zw">**Zishen Wan, Nandhini Chandramoorthy, Karthik Swaminathan, Pin-Yu Chen, Kshitij Bhardwaj, Vijay Janapa Reddi, Arijit Raychowdhury - MulBERRY: Enabling Bit-Error Robustness for Energy-Efficient Multi-Agent Autonomous Systems"**</a>

"The adoption of autonomous swarms, consisting of a multitude of unmanned aerial vehicles (UAVs), operating in a collaborative manner, has become prevalent in mainstream application domains for both military and civilian purposes. These swarms are expected to collaboratively carry out navigation tasks and employ complex reinforcement learning (RL) models within the stringent onboard size, weight, and power constraints. While techniques such as reducing on-board operating voltage can improve the energy efficiency of both computation and flight missions, they can lead to on-chip bit failures that are detrimental to mission safety and performance.

To this end, we propose MulBERRY, a multi-agent robust learning framework to enhance bit error robustness and energy efficiency for resource-constrained autonomous UAV swarms. MulBERRY supports multi-agent robust learning, both offline and on-device, with adaptive and collaborative agent-server optimizations. For the first time, MulBERRY demonstrates the practicality of robust low-voltage operation on multi-UAV systems leading to energy savings in both compute and mission quality-of-flight. We conduct extensive system-level experiments on autonomous multi-UAV navigation by leveraging algorithm-level robust learning techniques, and hardware-level bit error, thermal, and power characterizations. Through evaluations, we demonstrate that MulBERRY achieves robustness-performance-efficiency co- optimizations for autonomous UAV swarms. We also show that MulBERRY generalizes well across fault patterns, environments, UAV types, UAV agent numbers, and RL policies, with up to 18.97% reduction in flight energy, 22.07% increase in the number of successful missions, and 4.16× processing energy reduction."

<a id="cz">**Canlin Zhang, Gauthaman Murali, Sung-Kyu Lim, Tushar Krishna - Improving Scalability of Flexible On-chip Interconnect for DL Accelerators Using Logic-on-Logic 3D ICs"**</a>

Deep Learning (DL) Models are becoming more diverse in dimension, shapes, and sizes. To accommodate this trend, flexible interconnect designs have been introduced to DL accelerators to support flexible dataflow. This work identifies the scalability issues of flexible interconnect on 2D IC. We propose three systematic partitioning methods for converting 2D flexible interconnect to 2-tier, Logic-on-Logic 3D designs to improve throughput and energy efficiency. We implemented those methods on two flexible DL accelerator designs. Our EDA results show up to 3× improvement in design frequency and up to a 75% increase in theoretical throughput.

<a id="fmm">**Francisco Munoz Martinez, Michael Pellauer, Jose L Abellan, Manuel E Acacio, Tushar Krishna - Flexagon: A Reconfigurable Sparse Multi-dataflow Accelerator for DNNs"**</a>

Sparsity is a growing trend in modern DNN models. Existing Sparse-Sparse Matrix Multiplication (SpMSpM) accelerators are tailored to a particular SpMSpM dataflow (i.e., Inner Product, Outer Product or Gustavson’s), that determines theiroverall efficiency. We demonstrate that this static decision inherently results in a suboptimal dynamic solution. This is because different SpMSpM kernels show varying features (i.e.,dimensions, sparsity pattern, sparsity degree), which makes each dataflow better suited to different data sets. In this work we present Flexagon, the first SpMSpM reconfigurable accelerator that is capable of performing SpMSpM computation by using the particular dataflow that best matches each case. Flexagon accelerator is based on a novel Merger-Reduction Network (MRN) that unifies the concept of reducing and merging in the same substrate, increasing efficiency. Additionally, Flexagon also includes a 3-tier memory hierarchy, specifically tailored to the different access characteristics of the input and output compressed matrices. Using detailed cycle-level simulation of contemporary DNN models from a variety of application domains, we show that Flexagon achieves average performance benefits of 4.59x, 1.71x, and 1.35x with respect to the state-of-the-art SIGMA-like, Sparch-like and GAMMA-like accelerators (265% , 67% and 18%, respectively, in terms of average performance/area efficiency)

<a id="gm">**Girish Mururu, Sharjeel Khan, Bodhisatwa Chatterjee, Chao Chen, Chris Porter, Ada Gavrilovska, Santosh Pande - Beacons: An End-to-End Compiler Framework for Predicting and Utilizing Dynamic Loop Characteristics"**</a>

"Efficient management of shared resources is a critical problem in high-performance computing (HPC) environments. Tackling this problem in a scalable fashion is challenging due to the knowledge needed. The workload scheduler must possess in-depth knowledge about various application resource requirements and runtime phases at fine granularities.
In this work, we show that applications’ resource requirements and execution phase behavior can be captured in a scalable manner at runtime by estimating important program artifacts termed “dynamic loop characteristics”. We present Beacons Framework, an end-to-end compiler and scheduling framework, that estimates dynamic loop characteristics, encapsulates them in compiler-instrumented beacons in an application, and broadcasts them during application runtime, for proactive workload scheduling. Through a combination of compiler analysis and machine learning, we focus on estimating four important loop characteristics: loop trip-count, loop timing, loop memory footprint, and loop data-reuse behavior.
At the backend, Beacons Framework entails a proactive workload scheduler that leverages the runtime information to orchestrate aggressive process co-locations, for maximizing resource concurrency. Our results show that Beacons Framework can predict different loop characteristics with an accuracy of 85% to 95% on average, and the proactive scheduler obtains an average throughput improvement of 1.9x (up to 3.2x) over the state-of-the-art schedulers on an Amazon Graviton2 machine on consolidated workloads involving 1000-10000 co-executing processes, across 51 benchmarks."

<a id="mi">**Mikhail Isaev - Scaling Infrastructure to Support Multi-Trillion Parameter LLM Training"**</a>

This poster discusses efficient system designs for Large Language Model (LLM) scaling to up to 128 trillion parameters. We use a comprehensive analytical performance model to analyze how such models could be trained on current systems while maintaining 75% Model FLOPS Utilization (MFU). We first show how tensor offloading alone can be used to dramatically increase the size of trainable LLMs. We analyze performance bottlenecks when scaling on systems up to 16,384 GPUs and with models up to 128T parameters. Our findings suggest that current H100 GPUs with 80 GiB of HBM enabled with 512 GiB of tensor offloading capacity allows scaling to 11T-parameter LLMs; and getting to 128T parameters requires 120 GiB of HBM and 2 TiB of offloading memory, yielding 75%+ MFU, which is uncommon even when training much smaller LLMs today.

<a id="md">**Max Dabagia, Christos Papadimitriou, Santosh Vempala - Computation with Sequences of Assemblies"**</a>

Even as machine learning exceeds human-level performance on many applications, the generality, robustness, and rapidity of the brain's learning capabilities remain unmatched. How cognition arises from neural activity is &lt;em&gt; the&lt;/em&gt; central open question in neuroscience, inextricable from the study of intelligence itself. A simple formal model of neural activity was proposed in Papadimitriou (2020) and has been subsequently shown, through both mathematical proofs and simulations, to be capable of implementing certain simple cognitive operations via the creation and manipulation of assemblies of neurons. However, many intelligent behaviors rely on the ability to recognize, store, and manipulate temporal &lt;em&gt; sequences &lt;/em&gt; of stimuli (planning, language, navigation, to list a few). Here we show that, in the same model, time can be captured naturally as precedence through synaptic weights and plasticity, and, as a result, a range of computations on &lt;em&gt; sequences &lt;/em&gt; of assemblies can be carried out.  In particular, repeated presentation of a sequence of stimuli leads to the memorization of the sequence through corresponding neural assemblies: upon future presentation of any stimulus in the sequence, the corresponding assembly and its subsequent ones will be activated, one after the other, until the end of the sequence.  If the stimulus sequence is presented to two brain areas simultaneously, a scaffolded representation is created, resulting in more efficient memorization and recall, in agreement with cognitive experiments. Finally, we show that any finite state machine can be learned in a similar way, through the presentation of appropriate patterns of sequences.  Through an extension of this mechanism, the model can be shown to be capable of universal computation. We support our analysis with a number of experiments to probe the limits of learning in this model in key ways. Taken together, these results provide a concrete hypothesis for the basis of the brain's remarkable abilities to compute and learn, with sequences playing a vital role.

<a id="cj">**Christopher Jawetz, Fatima Chrit, Spencer Bryngelson, Alexander Alexeev - Developing Quantum Computing Algorithms for Modeling Heat Transfer with Phase Change"**</a>

Computational Fluid Dynamics (CFD) has long stood as one of the most demanding arenas in computational science, challenging the capabilities of conventional computing architectures with its scale. Despite the significant computational resources dedicated to CFD, the complexity and scale of simulations often necessitate trade-offs in simulation fidelity to achieve results within a practical timeframe. This constraint has fueled the pursuit of more efficient computational approaches, with quantum computing emerging as a particularly promising avenue. Quantum computing, with its ability to create and manipulate superpositions of multiple states simultaneously, offers the potential for exponential acceleration of CFD algorithms. In this context, we have pioneered a novel approach by developing a heat transfer algorithm using the lattice-Boltzmann method, which is well-suited for quantum computing due to its local interactions and inherent parallelism. We use this to investigate quantum spin interactions through a quantum Monte Carlo technique, which achieves a quadratic speedup compared to conventional computational methods. This innovative combination leverages the unique strengths of quantum computing and marks a significant step forward in overcoming the limitations of traditional CFD simulations, potentially transforming the field by enabling higher-fidelity simulations to be conducted more efficiently and rapidly.
