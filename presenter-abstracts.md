# Thursday, February 8th, 2024

## <a id="hf">Hal Finkel - _Keynote_ - The Department of Energy’s Advanced Scientific Computing Research Program: Recent Advances and Trends Motivating Future Research</a>

<img alt="Hal Finkel Headshot" src="./presenter_slides/Thursday_02_08_24/headshots/hf_headshot.jpeg" width="300" height="300">

<!--<a id="hf_ab"></a>**Abstract:** -->

<a id="hf_bio"></a>**Biography:**
Hal is a program manager for computer-science research in the US Department of Energy (DOE) Office of Science’s Advanced Scientific Computing Research (ASCR) program and is currently the acting Division Director for ASCR’s Computational Science Research and Partnership division. Prior to joining ASCR, Hal was the Lead for Compiler Technology and Programming Languages at Argonne’s Leadership Computing Facility. He graduated from Yale University in 2011 with a Ph.D. in theoretical physics focusing on numerical simulation of early-universe cosmology.


## <a id="hf">Alexey Tumanov - ML Systems</a>

<img alt="Alexey Tumanov Headshot" src="./presenter_slides/Thursday_02_08_24/headshots/alexey_tumanov_0.png" width="230" height="300">

<!--<a id="hf_ab"></a>**Abstract:** -->

<a id="hf_bio"></a>**Biography:**
I've started as a tenure-track Assistant Professor in the School of Computer Science at Georgia Tech in August 2019, transitioning from my postdoc at the University of California Berkeley, where I worked with Ion Stoica and collaborated closely with Joseph Gonzalez. I completed my Ph.D. at Carnegie Mellon University, advised by Gregory Ganger. At Carnegie Mellon, I was honored by the prestigious NSERC Alexander Graham Bell Canada Graduate Scholarship (NSERC CGS-D3) and partially funded by the Intel Science and Technology Centre for Cloud Computing and Parallel Data Lab. Prior to Carnegie Mellon, I worked on agile stateful VM replication with para-virtualization at the University of Toronto, where I worked with Eyal de Lara and Michael Brudno. My interest in cloud computing, datacenter operating systems, and programming the cloud brought me to the University of Toronto from industry, where I had been developing cluster middleware for distributed datacenter resource management.


## <a id="hf">Vijay Reddi - ML Systems</a>

<img alt="Vijay Reddi Headshot" src="./presenter_slides/Thursday_02_08_24/headshots/vijay.jpg" width="300" height="300">

<!--<a id="hf_ab"></a>**Abstract:** -->

<a id="hf_bio"></a>**Biography:**
My research is centered on mobile and edge-centric computing systems with a rare taste for cloud computing aspects, mostly as it pertains to edge computing or my students' interests. I direct the Edge Computing Lab. I believe in solving computing problems, rather than associating myself with a particular domain or field of computing (i.e., hardware or software). I take great pride in that, and that reflects in my research groups' training. I generally publish in Computer Architecture, Robotics and ML venues.

## <a id="hf">Santosh Vempala - "Food for Thought"</a>

<img alt="Santosh Vempala Headshot" src="./presenter_slides/Thursday_02_08_24/headshots/santosh_vempala_0.png" width="250" height="300">

<a id="hf_ab"></a>**Abstract:**
How does the mind (perception and cognition) arise from the brain (neurons and synapses)? We discuss a rigorous model of the brain that is consistent with basic principles of experimental neuroscience (inhibition, plasticity, homeostasis) and in which memorization, computation and learning are emergent phenomena --- consequences of exposure to sequences of stimuli.

<a id="hf_bio"></a>**Biography:**
Santosh Vempala is Frederick Storey II Chair of Computing and Distinguished Professor in the School of Computer Science, with courtesy appointments in the schools of Mathematics and Industrial and Systems Engineering (ISyE). He served as the founding director of the Algorithms and Randomness Center and ThinkTank (2006-2011), and is currently the director of GT's oldest interdisciplinary PhD program --- Algorithms, Combinatorics and Optimization. His research interests include algorithms, randomness, high-dimensional geometry, brain and computing-for-good (C4G). He graduated from CMU in 1997 advised by Avrim Blum and was on the MIT faculty until 2007 except for a year as a Miller Fellow at UC Berkeley. Vempala is also a Sloan, Guggenheim, ACM, AMS and generally excitable Fellow, especially when a phenomenon that appears complex from one perspective turns out to be simple from another. In recent years, he has been trying to understand the limits of sampling and optimization algorithms and building a computational theory of brain.

## <a id="hf">Prasanna Balaprakash - Plenary Talk </a>

<img alt="Prasanna Balaprakash Headshot" src="./presenter_slides/Thursday_02_08_24/headshots/BalaprakashProfile_0.jpg" width="300" height="300">

<a id="hf_ab"></a>**Abstract:**
Heterogeneous nodes, many-core processors, and deep memory hierarchies, along with power and energy demands, make application and system management on existing and emerging high-performance computing (HPC) platforms an increasingly daunting task. Current application portability strategies are mostly static and are often based on a narrow set of workloads. Consequently, such a one-size-fits-all approach will render several existing, emerging, and future HPC platforms less performant for a wide range of scientific applications—a significant hurdle for data-intensive scientific discovery. In this talk, we will discuss the challenges associated with performance portability/tuning, data movement, and data storage in Department of Energy (DOE) leadership class HPC systems. We will present data-efficient machine learning methodologies that we have developed to effectively address some of these challenges. Finally, we will give a high-level overview of the DOE Frontiers in Artificial Intelligence for Science, Security, and Technology (FASST) initiative, which seeks to develop AI technologies for scientific and security applications through exascale and post-exascale leadership computing capabilities.

<a id="hf_bio"></a>**Biography:**
Prasanna Balaprakash is the Director of AI Programs and Distinguished R&D Scientist at Oak Ridge National Laboratory, where he directs laboratory research, development and application of artificial intelligence and machine learning (AI/ML) to solve problems of national importance. His research interests span artificial intelligence, machine learning, optimization, and high-performance computing. He is a recipient of the U.S. Department of Energy's 2018 Early Career Award. Prior to joining Oak Ridge, he was a R&D lead and computer scientist at Argonne National Laboratory. He earned his Ph.D. from CoDE-IRIDIA at the Université Libre de Bruxelles in Brussels, Belgium, where he was a recipient of the European Commission's Marie Curie and Belgian F.R.S-FNRS Aspirant fellowships.

## <a id="hf">Josiah Hester - Extreme Low Power Computing</a>

<img alt="Hal Finkel Headshot" src="./presenter_slides/Thursday_02_08_24/headshots/josiah_hester.jpg" width="300" height="300">

<!--<a id="hf_ab"></a>**Abstract:** -->

<a id="hf_bio"></a>**Biography:**
I direct Ka Moamoa at Georgia Tech, a research lab exploring energy efficient computing in the context of global scale applications. We explore and develop new hardware designs, systems, interaction techniques, tools, and programming abstractions so that anyone can easily design, debug, and deploy tiny energy harvesting computers that work despite frequent power failures, constrained resources, and unpredictable conditions.


I work towards a sustainable future for computing informed by my Native Hawaiian (Kānaka Maoli) heritage. I apply my work in the field of intermittent computing to large-scale sensing for sustainability and conservation, health wearables, and interactive devices. My work is supported by multiple grants from the NSF, NIH, ARPA-H, DARPA, DoD, 3M, VMware, and the Sloan Foundation.

I was named a Sloan Fellow in Computer Science, won my NSF CAREER and a VMware Early Career Faculty Award in 2022. I was named to Popular Science’s Brilliant 10, the AISES Most Promising Scientist, and won a 3M Non-Tenured Faculty Award in 2021.

## <a id="hf">Azad Naeemi - Extreme Low Power Computing</a>

<img alt="Azad Naeemi Headshot" src="./presenter_slides/Thursday_02_08_24/headshots/Azad.webp" width="300" height="300">

<a id="hf_ab"></a>**Abstract:**
Novel Ternary Content Addressable Memory Designs Based on Emerging Magnetic and Magnetoelectric Devices
 
With the ever-growing limits imposed by interconnects, novel computing paradigms that may reduce data traffic between logic and memory have become more attractive. Various in-memory computing approaches are therefore being explored for various applications/domains such as neural networks, associative memories, spin- torque nano-oscillators, probabilistic computing, and reservoir computing. Ternary content addressable memory (TCAM) is an associative memory that performs parallel data searches over a memory array and outputs if/where a match occurs. TCAMs have been used in a variety of applications, such as few-shot learning, DNA read alignment, deep random forest, and hyperdimensional computing. In this talk, compact TCAM cells with only a few transistors based on spin-orbit-torque and magnetoelectric devices are proposed and their potential performances are quantified using experimentally validated/calibrated physical models.

<a id="hf_bio"></a>**Biography:**
Azad Naeemi is a professor in the School of Electrical and Computer Engineering at the Georgia Institute of Technology. His technical research crosses the boundaries of materials, devices, circuits, and systems, investigating integrated circuits based on conventional and emerging nanoscale devices and interconnects. His educational research includes experiential learning environments and their impact on conceptual understanding of scientific and engineering topics. He serves as the Editor-in-Chief of the IEEE Journal on Exploratory Computational Devices and Circuits (JXCDC) and is the Associate Director for Computation for the NSF-supported National Nanotechnology Coordinated Infrastructure (NNCI). He is a recipient of the IEEE Electron Devices Society (EDS) Paul Rappaport Award, the Inaugural IEEE Solid-State Circuits Society (SSCS) James Meindl Innovators Award, an NSF CAREER Award, an SRC Inventor Recognition Award, and several best paper awards from international conferences. 
 

## <a id="hf">Jason Lowe-Power - Simulation and Testbeds</a>

<img alt="Jason Lowe-Power Headshot" src="./presenter_slides/Thursday_02_08_24/headshots/jason.jpg" width="270" height="300">

<a id="hf_ab"></a>**Abstract:**
Computer architecture simulation is a vital tool for designing and evaluating the next generation of supercomputers. However, simulation is not a trivial task, as it requires balancing accuracy, speed, flexibility, and scalability. In this talk, I discuss a case study in the success of an open-source community: the gem5 simulator which has been widely used and contributed by the computer architecture research community for over two decades. I will show how gem5 enables cutting-edge research and innovation in supercomputing by highlighting some recent work from my group. I will also discuss our ongoing work in modeling the next generation of supercomputers. Finally, I will talk about how to build a successful open-source community around a research tool and what has made gem5 such a success story.

<a id="hf_bio"></a>**Biography:**
Jason Lowe-Power is an Associate Professor at University of California, Davis where he leads the Davis Computer Architecture Research Lab (DArchR). His research interests include optimizing data movement in heterogeneous systems, hardware support for security, and simulation infrastructure. Professor Lowe-Power is also the Chair of the Project Management Committee for the gem5 open-source simulation infrastructure. He received his PhD in 2017 from the University of Wisconsin, Madison, and received an NSF CAREER Award and a Google Research Scholar Award

## <a id="hf">Jeffrey Young - Simulation and Testbeds</a>

<img alt="Jeff Young Headshot" src="./presenter_slides/Thursday_02_08_24/headshots/jeffrey_young.png" width="280" height="300">

<!--<a id="hf_ab"></a>**Abstract:** -->

<a id="hf_bio"></a>**Biography:**
Jeffrey (Jeff) Young is a research scientist in Georgia Tech's School of Computer Science. His main research interests include investigating scheduling and data movement for accelerators like GPU and Xeon Phi and working to model and map algorithms to high-performance architectures. He is currently working on a collaborative research program that is focused on mapping bandwidth-intensive algorithms to 3D stacked memories like Hybrid Memory Cube (HMC) and High Bandwidth Memory (HBM)  and on performing near-memory computation on devices like FPGAs and GPUs. He received his PhD in computer engineering in 2013 from Georgia Tech's ECE department.

## <a id="hf">Murali Emani -Simulation and Testbeds </a>

<img alt="Murali Emani Headshot" src="./presenter_slides/Thursday_02_08_24/headshots/Murali-profile-pic.jpg" width="300" height="300">

<!--<a id="hf_ab"></a>**Abstract:** -->

<a id="hf_bio"></a>**Biography:**
I am a Computer Scientist in the Artificial Intelligence and Machine Learning (AIML) group with the Argonne Leadership Computing Facility (ALCF) at Argonne National Laboratory. Prior, I was a Postdoctoral Research Staff Member at Lawrence Livermore National Laboratory, US. I obtained my PhD and worked as a Research Associate at the Institute for Computing Systems Architecture at the School of Informatics, University of Edinburgh, UK under the guidance of Prof. Michael O'Boyle. My research interests are in Parallel programming models, High Performance Computing, Scalable Machine Learning, Runtime Systems, Emerging HPC architectures, Online Adaptation. Some of my current projects include:

1. Developing performance models to identifying and addressing bottlenecks while scaling machine learning and deep learning frameworks on emerging supercomputers for scientific applications.
2. Co-design of emerging hardware architectures to scale up machine learning algorithms.
3. Efforts on benchmarking ML/DL frameworks and methods on HPC systems.


# Friday, February 9th, 2024
## <a id="hf">Yiran Chen - Keynote</a>

<img alt="Yiran Chen Headshot" src="./presenter_slides/Friday_02_09_24/headshots/yiran.jpg" width="300" height="300">

<a id="hf_ab"></a>**Abstract:**
As artificial intelligence (AI) transforms various industries, state-of-the-art models have grown exponentially in size and capability. However, previous optimization efforts have primarily concentrated on computational aspects, often overlooking the significant bottleneck in AI system efficiency caused by the storage, retrieval, and orchestration of data. To address this challenge, we adopt a data-centric approach, performing collaborative optimization across the algorithms, systems, architecture, and circuit layers. In this presentation, we will first discuss the memory capacity and bandwidth bottleneck that has emerged with the advancement of AI models. Furthermore, we will present our optimization efforts aimed at addressing this bottleneck, which include compressing the AI model, tailoring the computation schedule, and customizing the memory hierarchy. Additionally, we explore the potential of compute-in-memory as a comprehensive solution to these challenges, presenting a holistic approach that integrates computation and memory for enhanced efficiency in AI systems. At the conclusion of our presentation, we will share our insights and vision for a data-centric approach to optimizing AI systems.

<a id="hf_bio"></a>**Biography:**
Yiran Chen received B.S. (1998) and M.S. (2001) from Tsinghua University and Ph.D. (2005) from Purdue University. After five years in the industry, he joined the University of Pittsburgh in 2010 as Assistant Professor and was promoted to Associate Professor with tenure in 2014, holding Bicentennial Alumni Faculty Fellow. He is now the John Cocke Distinguished Professor of Electrical and Computer Engineering at Duke University and serving as the director of the NSF AI Institute for Edge Computing Leveraging the Next-generation Networks (Athena), the NSF Industry-University Cooperative Research Center (IUCRC) for Alternative Sustainable and Intelligent Computing (ASIC), and the co-director of Duke Center for Computational Evolutionary Intelligence (DCEI). His group focuses on the research of new memory and storage systems, machine learning and neuromorphic computing, and mobile computing systems. Dr. Chen has published 1 book and about 600 technical publications and has been granted 96 US patents. He has served as the associate editor of more than a dozen international academic periodicals and served on the technical and organization committees of about 70 international conferences. He is now serving as the Editor-in-Chief of the IEEE Circuits and Systems Magazine. He received 11 best paper awards, 1 best poster award, and 15 best paper nominations from international conferences and workshops. He received numerous awards for his technical contributions and professional services such as the IEEE CASS Charles A. Desoer Technical Achievement Award, the IEEE Computer Society Edward J. McCluskey Technical Achievement Award, etc. He has been the distinguished lecturer of IEEE CEDA and CAS. He is a Fellow of the AAAS, ACM, and IEEE, and now serves as the chair of ACM SIGDA.

## <a id="hf">Ben Feinberg - Analog and Neuromorphic</a>

<img alt="Ben Feinberg Headshot" src="./presenter_slides/Friday_02_09_24/headshots/ben.jpg" width="300" height="300">

<a id="hf_ab"></a>**Abstract:**
Over the past decade there has been a resurgence of interest in analog accelerators capitalizing on developments in dense, CMOS-compatible, and programmable non-volatile memories. Taking advantage of fundamental circuit properties to perform linear algebra calculations in the analog domain has the potential to avoid major data movement challenges in modern systems. Unfortunately, a decade on analog linear algebra accelerators still seem closer to academic research than real products. This talk will discuss some of the challenges and pitfalls in architecture research on analog accelerators which have contributed to the slow progress. Despite these challenges however, there have been significant advances in analog accelerators and the field is well positioned for architectural research. This talk will review some recent results showing the potential of analog accelerators for a range of applications including computer vision, signal processing, and optimization, and discuss important architecture research directions for analog accelerators.

<a id="hf_bio"></a>**Biography:**
Ben Feinberg is a Senior Member of Technical Staff in the Scalable Computer Architecture Group at Sandia National Laboratories. His research focuses on architectures for autonomous system with an emphasis on analog accelerators. Dr. Feinberg leads architecture modeling and system software research for Sandia's Rad-Edge project, and is Sandia's lead architect for the DARPA OPTIMA program. He is one of the developers of CrossSim, and leads a project with DOE Vehicle Technology Office on compute requirements for autonomous vehicles. Prior to joining Sandia in 2019, Ben completed his PhD in Electrical Engineering at the University of Rochester

## <a id="hf">Jennifer Hasler - Analog and Neuromorphic</a>

<img alt="Jennifer Hasler Headshot" src="./presenter_slides/Friday_02_09_24/headshots/JenniferHasler.webp" width="300" height="300">

<!--<a id="hf_ab"></a>**Abstract:** -->

<a id="hf_bio"></a>**Biography:**
Rev. Dr. Jennifer Hasler received her B.S.E. and M.S. degrees in electrical engineering from Arizona State University in August 1991. She received her Ph.D. in computation and neural systems from California Institute of Technology in February 1997, and she received her Master of Divinity degree from the Candler School of Theology at Emory University in 2020. Her ordination service was held on December 17, 2021. 

Dr. Hasler is a professor at the Georgia Institute of Technology in the School of Electrical and Computer Engineering; Atlanta is the coldest climate in which Dr. Hasler has lived.

Dr. Hasler founded the Integrated Computational Electronics (ICE) laboratory at Georgia Tech, a laboratory affiliated with the Laboratories for Neural Engineering.

Dr. Hasler is a member of Tau Beta P, Eta Kappa Nu, and the IEEE.

## <a id="hf">Spencer Bryngelson - Emerging Applications</a>

<img alt="Spencer Bryngelson Headshot" src="./presenter_slides/Friday_02_09_24/headshots/spencer_bryngelson.jpg" width="280" height="300">

<a id="hf_ab"></a>**Abstract:** 
Partial differential equation (PDE) solvers for Navier-Stokes fluids problems are important for many science and engineering problems. Quantum algorithms have been verified to can solve linear PDEs and verified on simulators. Fault-tolerant quantum hardware may make exponential speedups possible for, for example via linearization methods and HHL-type linear solvers. Still, NISQ-era near-term quantum hardware requires algorithms that demand less quantum volume: shallower gate depths and fewer qubits. Variational algorithms, like VQE and VQLS, are appropriate under such restrictions. Here, we present work on a hybrid approach to solving the nonlinear incompressible Navier-Stokes equations. A classical computer performs the nonlinear computations and a quantum algorithm, on simulator or hardware, performs the cumbersome Poisson equation solve that enforces mass continuity. A lid-driven cavity flow problem is investigated to analyze sensitivity to variational parameters and quantum noise.

<a id="hf_bio"></a>**Biography:**
Spencer Bryngelson is a tenure-track assistant professor in the College of Computing at Georgia Tech. Previously, he was a senior postdoctoral researcher at Caltech (with Professor Tim Colonius). He has been a visiting researcher at MIT (with Professor Themis Sapsis) and a postdoctoral researcher at the Center for Exascale Simulation of Plasma-Coupled Combustion (XPACC). He received his Ph.D. and M.S. in Theoretical and Applied Mechanics from the University of Illinois at Urbana–Champaign in 2017 and 2015, working with Professor Jonathan Freund. In 2013, he received B.S. degrees in both Mechanical Engineering and Mathematics from the University of Michigan–Dearborn.

## <a id="hf">Mathias Jacquelin - Emerging Applications</a>

<img alt="Mathias Jacquelin Headshot" src="./presenter_slides/Friday_02_09_24/headshots/mathias.jpeg" width="280" height="300">

<!-- <a id="hf_ab"></a>**Abstract:** 
Partial differential equation (PDE) solvers for Navier-Stokes fluids problems are important for many science and engineering problems. Quantum algorithms have been verified to can solve linear PDEs and verified on simulators. Fault-tolerant quantum hardware may make exponential speedups possible for, for example via linearization methods and HHL-type linear solvers. Still, NISQ-era near-term quantum hardware requires algorithms that demand less quantum volume: shallower gate depths and fewer qubits. Variational algorithms, like VQE and VQLS, are appropriate under such restrictions. Here, we present work on a hybrid approach to solving the nonlinear incompressible Navier-Stokes equations. A classical computer performs the nonlinear computations and a quantum algorithm, on simulator or hardware, performs the cumbersome Poisson equation solve that enforces mass continuity. A lid-driven cavity flow problem is investigated to analyze sensitivity to variational parameters and quantum noise.
-->
<a id="hf_bio"></a>**Biography:**
Mathias Jacquelin is a Senior Member of the Technical Staff at Cerebras Systems, focusing on parallel and distributed programming techniques for Machine Learning and High-Performance Computing applications. Before joining Cerebras, Mathias was a Research Staff Scientist at the Lawrence Berkeley National Laboratory, where he worked on task-based scheduling and lightweight one-sided communications in the context of sparse matrix direct solvers. He holds a PhD in Computer Science from the École Normale Supérieure de Lyon and an MSc in Information Technology from the Institut National des Sciences Appliquées de Lyon (INSA Lyon)., focusing on

Mathias has made various contributions to dense and sparse matrix and stencil computations. He received the 2017 Best Paper Award at the IXPUG workshop of the ISC HPC conference and the 2011 TCPP Best Poster award at the 25th International Parallel and Distributed Processing Symposium (IPDPS’11).

## <a id="hf">Larry Heck - Emerging Applications</a>

<img alt="Larry Heck Headshot" src="./presenter_slides/Friday_02_09_24/headshots/larry.jpeg" width="240" height="300">

<a id="hf_ab"></a>**Abstract:** 
With the very recent and broad awareness of Large Language Models (LLMs) across many disciplines and industries, there is a high level of excitement and expectation for the potential of this technology. As with any significant advance in technology, there is also the accompanying hype with overstatements of the capabilities of LLMs (e.g., reasoning, intelligence, emerging consciousness) as well as accompanying fear of what this advance represents in the broader AI story - fear of AI taking jobs, fear of AI misinformation, bias, undue influence on the political and economic processes, fear of unchecked autonomous AI, etc. In this talk, I will attempt to provide clarity on the reality of LLMs - a brief historical perspective on where they came from, what problems they were designed to solve, and what are their inherent limitations. I will discuss some of the more surprising capabilities that have emerged in LLMs, the potential of LLMs, and where the technology is going in the near future.

<a id="hf_bio"></a>**Biography:**
Larry Heck is a Professor in ECE and Interactive Computing, co-Head of the AI Hub and Machine Learning Center, Farmer Chair of Advanced Computing Concepts, and a GRA Eminent Scholar at the Georgia Institute of Technology. He is a Fellow of the IEEE, inducted into the Academy of Distinguished Engineers at Georgia Tech, and named a Distinguished Engineer at Texas Tech. After receiving the PhD EE from Georgia Tech, he joined SRI, followed by VP of R&D at Nuance, VP of Search and Advertising at Yahoo!, Chief Speech Scientist and Distinguished Engineer at Microsoft, Principal Scientist with Google Research, and CEO of Viv Labs and SVP at Samsung.

## <a id="hf">Roberto Gioiosa - HW-SW Co-Design</a>

<img alt="Roberto Gioiosa Headshot" src="./presenter_slides/Friday_02_09_24/headshots/Gioiosa.jpg" width="420" height="300">

<!--<a id="hf_ab"></a>**Abstract:** -->

<a id="hf_bio"></a>**Biography:**
Dr. Roberto Gioiosa is a senior research scientist and team lead at the Pacific Northwest National Laboratory (PNNL) in the High-Performance Computing Group. His research interests include operating systems and runtimes, high-performance computer architectures, memory, and networks, parallel and distributed programming models, resilience, performance and power modeling and analysis, and embedded systems.

Dr. Gioiosa earned his Ph.D. in 2006 from the University of Rome "Tor Vergara", Rome Italy. He has worked at the Los Alamos National Laboratory (LANL) (2004-2005), the Barcelona Supercomputing Center (BSC) (2006-2008 and 2009-2012), the IBM T.J. Watson Research Center (2008-2009) where he contributed to the development of the Compute Node Kernel for BG/Q systems, and Oak Ridge National Laboratory (ORNL) (2017-2018).

Currently, his projects include the development of system software for extremely heterogeneous systems, system software for scalable distributed systems, , DSSoC design, evaluation of emerging architecture and technologies for exascale systems and applications, and development of operating systems for exascale systems. He is a member of the ACM and IEEE Computer Society.


## <a id="hf">Bahar Asgari - HW-SW Co-Design</a>

<img alt="Bahar Asgari Headshot" src="./presenter_slides/Friday_02_09_24/headshots/bahar.jpg" width="300" height="300">

<!--<a id="hf_ab"></a>**Abstract:** -->

<a id="hf_bio"></a>**Biography:**
Bahar Asgari is an assistant professor in the Department of Computer Science at the University of Maryland, College Park (UMD), with a joint appointment at UMIACS and an affiliation with ECE department. Prior to UMD, she spent a year on Google's Systems and Services Infrastructure team, focusing on improving the performance of Google's systems and establishing strategies to balance various trade-offs, particularly in optimizing memory utilization, cost, and reliability. Asgari earned her Ph.D. in electrical and computer engineering from Georgia Tech in 2021. During her PhD, she was selected as a Rising Star in EECS in 2019 . Asgari’s doctoral dissertation, in consultation with her advisors Prof. Yalamanchili and Prof. Kim, focuses on efficiently improving the performance of sparse problems. Asgari is a recipient of the DoE Early Career Award 2023.
